{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b37df35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, matthews_corrcoef, accuracy_score, precision_score, recall_score, f1_score, fbeta_score, confusion_matrix, make_scorer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from IPython.display import FileLink, FileLinks, clear_output\n",
    "from IPython.display import FileLink, FileLinks\n",
    "from xgboost import XGBRegressor\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bba23d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\rrava\\OneDrive\\Documents\\01 - Fall 2022\\Research Project\\Clean Datasets\\cleaned_ANL_with_waiting_times_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc465434",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Runtime models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d4ac10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.1\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits = 3)\n",
    "\n",
    "beta = 0.25\n",
    "fbeta_scorer = make_scorer(fbeta_score, beta=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a7ecfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_time = 15 * 60\n",
    "X = df[['user_id', 'queue_number', 'submit_time', 'requested_time', 'requested_CPUs']]\n",
    "y = df[['runtime']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af24b86a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30b7b3d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor(max_depth=10, max_features='sqrt', min_samples_leaf=50,\n",
      "                      min_samples_split=100, n_estimators=250, random_state=0)\n",
      "MCC: 0.5733255289985368\n",
      "F1 score: 0.8524624157594608\n",
      "Precision score: 0.7584870848708487\n",
      "Recall score: 0.9730177514792899\n",
      "FBeta score: 0.7684534608829512\n"
     ]
    }
   ],
   "source": [
    "# Runtimes - Regression - Random Forest\n",
    "\n",
    "## Hyperparameter tuning with cross-validation\n",
    "\n",
    "cv_clf = RandomForestRegressor()\n",
    "param_search = {'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2'], 'max_depth': [5, 10, 15], 'min_samples_leaf': [50, 100, 250], 'min_samples_split': [100, 200, 500], 'random_state': [0]}\n",
    "gsearch = GridSearchCV(estimator=cv_clf, cv=tscv, param_grid=param_search, scoring='neg_mean_absolute_error')\n",
    "gsearch.fit(X_train, y_train.values.ravel())\n",
    "print(gsearch.best_estimator_)\n",
    "\n",
    "## Test set evaluation\n",
    "\n",
    "y_pred = gsearch.predict(X_test)\n",
    "\n",
    "y_test_binary = y_test['runtime'] > threshold_time\n",
    "y_pred_binary = y_pred > threshold_time\n",
    "\n",
    "print(\"MCC: \" + str(matthews_corrcoef(y_test_binary, y_pred_binary)))\n",
    "print(\"F1 score: \" + str(f1_score(y_test_binary, y_pred_binary)))\n",
    "print(\"Precision score: \" + str(precision_score(y_test_binary, y_pred_binary)))\n",
    "print(\"Recall score: \" + str(recall_score(y_test_binary, y_pred_binary)))\n",
    "print(\"FBeta score: \" + str(fbeta_score(y_test_binary, y_pred_binary, beta=beta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e8da02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression()\n",
      "MCC: 0.1614363921064019\n",
      "F1 score: 0.7530499803227076\n",
      "Precision score: 0.644384576528035\n",
      "Recall score: 0.9057988165680473\n",
      "FBeta score: 0.6555129018932181\n"
     ]
    }
   ],
   "source": [
    "# Runtimes - Regression - Linear Regression\n",
    "\n",
    "## Hyperparameter tuning with cross-validation\n",
    "\n",
    "cv_clf = LinearRegression()\n",
    "param_search = {}\n",
    "gsearch = GridSearchCV(estimator=cv_clf, cv=tscv, param_grid=param_search, scoring='neg_mean_absolute_error')\n",
    "gsearch.fit(X_train, y_train.values.ravel())\n",
    "print(gsearch.best_estimator_)\n",
    "\n",
    "## Test set evaluation\n",
    "\n",
    "y_pred = gsearch.predict(X_test)\n",
    "\n",
    "y_test_binary = y_test['runtime'] > threshold_time\n",
    "y_pred_binary = y_pred > threshold_time\n",
    "\n",
    "print(\"MCC: \" + str(matthews_corrcoef(y_test_binary, y_pred_binary)))\n",
    "print(\"F1 score: \" + str(f1_score(y_test_binary, y_pred_binary)))\n",
    "print(\"Precision score: \" + str(precision_score(y_test_binary, y_pred_binary)))\n",
    "print(\"Recall score: \" + str(recall_score(y_test_binary, y_pred_binary)))\n",
    "print(\"FBeta score: \" + str(fbeta_score(y_test_binary, y_pred_binary, beta=beta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a62ee589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingRegressor(max_features='sqrt', min_samples_leaf=250,\n",
      "                          min_samples_split=100, random_state=0)\n",
      "MCC: 0.5937210486886912\n",
      "F1 score: 0.8542108198549916\n",
      "Precision score: 0.8078059071729958\n",
      "Recall score: 0.9062721893491125\n",
      "FBeta score: 0.8130019359270593\n"
     ]
    }
   ],
   "source": [
    "# Runtimes - Regression - Gradient Boosting\n",
    "\n",
    "## Hyperparameter tuning with cross-validation\n",
    "\n",
    "cv_clf = GradientBoostingRegressor()\n",
    "param_search = {'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2'], 'min_samples_leaf': [50, 100, 250], 'min_samples_split': [100, 200, 500], 'random_state': [0]}\n",
    "gsearch = GridSearchCV(estimator=cv_clf, cv=tscv, param_grid=param_search, scoring='neg_mean_absolute_error')\n",
    "gsearch.fit(X_train, y_train.values.ravel())\n",
    "print(gsearch.best_estimator_)\n",
    "\n",
    "## Test set evaluation\n",
    "\n",
    "y_pred = gsearch.predict(X_test)\n",
    "\n",
    "y_test_binary = y_test['runtime'] > threshold_time\n",
    "y_pred_binary = y_pred > threshold_time\n",
    "\n",
    "print(\"MCC: \" + str(matthews_corrcoef(y_test_binary, y_pred_binary)))\n",
    "print(\"F1 score: \" + str(f1_score(y_test_binary, y_pred_binary)))\n",
    "print(\"Precision score: \" + str(precision_score(y_test_binary, y_pred_binary)))\n",
    "print(\"Recall score: \" + str(recall_score(y_test_binary, y_pred_binary)))\n",
    "print(\"FBeta score: \" + str(fbeta_score(y_test_binary, y_pred_binary, beta=beta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8a0e31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ad4e561",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fc262d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['should_wait_runtime_actual'] = df['runtime'] > threshold_time\n",
    "y = df[['should_wait_runtime_actual']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29425a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24022251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(max_depth=15, min_samples_leaf=50, min_samples_split=500,\n",
      "                       random_state=0)\n",
      "MCC: 0.6151297113679481\n",
      "F1 score: 0.8647663951993142\n",
      "Precision score: 0.7900920305463089\n",
      "Recall score: 0.9550295857988166\n",
      "FBeta score: 0.7982010077149538\n"
     ]
    }
   ],
   "source": [
    "# Runtimes - Classification - Random Forest\n",
    "\n",
    "## Hyperparameter tuning with cross-validation\n",
    "\n",
    "cv_clf = RandomForestClassifier()\n",
    "param_search = {'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2'], 'max_depth': [5, 10, 15], 'min_samples_leaf': [50, 100, 250], 'min_samples_split': [100, 200, 500], 'random_state': [0]}\n",
    "gsearch = GridSearchCV(estimator=cv_clf, cv=tscv, param_grid=param_search, scoring=fbeta_scorer)\n",
    "gsearch.fit(X_train, y_train.values.ravel())\n",
    "print(gsearch.best_estimator_)\n",
    "\n",
    "## Test set evaluation\n",
    "\n",
    "y_pred = gsearch.predict(X_test)\n",
    "\n",
    "print(\"MCC: \" + str(matthews_corrcoef(y_test, y_pred)))\n",
    "print(\"F1 score: \" + str(f1_score(y_test, y_pred)))\n",
    "print(\"Precision score: \" + str(precision_score(y_test, y_pred)))\n",
    "print(\"Recall score: \" + str(recall_score(y_test, y_pred)))\n",
    "print(\"FBeta score: \" + str(fbeta_score(y_test, y_pred, beta=beta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f29fccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(max_features='sqrt', min_samples_leaf=50,\n",
      "                           min_samples_split=500, random_state=0)\n",
      "MCC: 0.5967221615488132\n",
      "F1 score: 0.8563212034067028\n",
      "Precision score: 0.8037790697674418\n",
      "Recall score: 0.9162130177514793\n",
      "FBeta score: 0.809623405223853\n"
     ]
    }
   ],
   "source": [
    "# Runtimes - Classification - Gradient Boosting\n",
    "\n",
    "## Hyperparameter tuning with cross-validation\n",
    "\n",
    "cv_clf = GradientBoostingClassifier()\n",
    "param_search = {'n_estimators': [100], 'max_features': ['sqrt'], 'min_samples_leaf': [50, 100, 250], 'min_samples_split': [100, 200, 500], 'random_state': [0]}\n",
    "gsearch = GridSearchCV(estimator=cv_clf, cv=tscv, param_grid=param_search, scoring=fbeta_scorer)\n",
    "gsearch.fit(X_train, y_train.values.ravel())\n",
    "print(gsearch.best_estimator_)\n",
    "\n",
    "## Test set evaluation\n",
    "\n",
    "y_pred = gsearch.predict(X_test)\n",
    "\n",
    "print(\"MCC: \" + str(matthews_corrcoef(y_test, y_pred)))\n",
    "print(\"F1 score: \" + str(f1_score(y_test, y_pred)))\n",
    "print(\"Precision score: \" + str(precision_score(y_test, y_pred)))\n",
    "print(\"Recall score: \" + str(recall_score(y_test, y_pred)))\n",
    "print(\"FBeta score: \" + str(fbeta_score(y_test, y_pred, beta=beta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c81ae00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.1, max_iter=1000, random_state=0)\n",
      "MCC: 0.5522736161851266\n",
      "F1 score: 0.8454169607744277\n",
      "Precision score: 0.7364722417427969\n",
      "Recall score: 0.9921893491124261\n",
      "FBeta score: 0.7478094798367211\n"
     ]
    }
   ],
   "source": [
    "# Runtimes - Classification - Logistic Regression\n",
    "\n",
    "## Hyperparameter tuning with cross-validation\n",
    "\n",
    "cv_clf = LogisticRegression()\n",
    "param_search = {'max_iter': [1000, 10000], 'C': [0.1, 0.01, 0.001, 0.0001], 'random_state': [0]}\n",
    "gsearch = GridSearchCV(estimator=cv_clf, cv=tscv, param_grid=param_search, scoring=fbeta_scorer)\n",
    "gsearch.fit(X_train, y_train.values.ravel())\n",
    "print(gsearch.best_estimator_)\n",
    "\n",
    "## Test set evaluation\n",
    "\n",
    "y_pred = gsearch.predict(X_test)\n",
    "\n",
    "print(\"MCC: \" + str(matthews_corrcoef(y_test, y_pred)))\n",
    "print(\"F1 score: \" + str(f1_score(y_test, y_pred)))\n",
    "print(\"Precision score: \" + str(precision_score(y_test, y_pred)))\n",
    "print(\"Recall score: \" + str(recall_score(y_test, y_pred)))\n",
    "print(\"FBeta score: \" + str(fbeta_score(y_test, y_pred, beta=beta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08ec1636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(n_neighbors=50)\n",
      "MCC: 0.0\n",
      "F1 score: 0.7599604280960518\n",
      "Precision score: 0.6128517551494053\n",
      "Recall score: 1.0\n",
      "FBeta score: 0.6271337390529909\n"
     ]
    }
   ],
   "source": [
    "# Runtimes - Classification - Nearest Neighbors\n",
    "\n",
    "## Hyperparameter tuning with cross-validation\n",
    "\n",
    "cv_clf = KNeighborsClassifier()\n",
    "param_search = {'n_neighbors': [5, 10, 15, 25, 50]}\n",
    "gsearch = GridSearchCV(estimator=cv_clf, cv=tscv, param_grid=param_search, scoring=fbeta_scorer)\n",
    "gsearch.fit(X_train, y_train.values.ravel())\n",
    "print(gsearch.best_estimator_)\n",
    "\n",
    "## Test set evaluation\n",
    "\n",
    "y_pred = gsearch.predict(X_test)\n",
    "\n",
    "print(\"MCC: \" + str(matthews_corrcoef(y_test, y_pred)))\n",
    "print(\"F1 score: \" + str(f1_score(y_test, y_pred)))\n",
    "print(\"Precision score: \" + str(precision_score(y_test, y_pred)))\n",
    "print(\"Recall score: \" + str(recall_score(y_test, y_pred)))\n",
    "print(\"FBeta score: \" + str(fbeta_score(y_test, y_pred, beta=beta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edb7fea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB()\n",
      "MCC: 0.39883627179129255\n",
      "F1 score: 0.6535061873895109\n",
      "Precision score: 0.8653921186110027\n",
      "Recall score: 0.5249704142011834\n",
      "FBeta score: 0.8335949417460703\n"
     ]
    }
   ],
   "source": [
    "# Runtimes - Classification - Naive Bayes\n",
    "\n",
    "## Hyperparameter tuning with cross-validation\n",
    "\n",
    "cv_clf = GaussianNB()\n",
    "param_search = {}\n",
    "gsearch = GridSearchCV(estimator=cv_clf, cv=tscv, param_grid=param_search, scoring=fbeta_scorer)\n",
    "gsearch.fit(X_train, y_train.values.ravel())\n",
    "print(gsearch.best_estimator_)\n",
    "\n",
    "## Test set evaluation\n",
    "\n",
    "y_pred = gsearch.predict(X_test)\n",
    "\n",
    "print(\"MCC: \" + str(matthews_corrcoef(y_test, y_pred)))\n",
    "print(\"F1 score: \" + str(f1_score(y_test, y_pred)))\n",
    "print(\"Precision score: \" + str(precision_score(y_test, y_pred)))\n",
    "print(\"Recall score: \" + str(recall_score(y_test, y_pred)))\n",
    "print(\"FBeta score: \" + str(fbeta_score(y_test, y_pred, beta=beta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41480588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a2fd49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Waiting Time Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89512c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['total_remaining_runtime'] = df['running_job_requested_wallclock_limit'] - df['elapsed_runtime_total']\n",
    "df['average_remaining_runtime'] = df['running_job_mean_wallclock_limit'] - df['elapsed_runtime_mean']\n",
    "\n",
    "X = df[['total_remaining_runtime', 'average_remaining_runtime', 'user_id', 'queue_number', 'submit_time', 'requested_time', 'requested_CPUs', 'num_running_jobs', 'num_waiting_jobs', 'running_job_mean_CPU_time', 'running_job_requested_wallclock_limit', 'running_job_mean_wallclock_limit', 'waiting_job_requested_CPU_time', 'waiting_job_mean_CPU_time', 'waiting_job_requested_wallclock_limit', 'waiting_job_mean_wallclock_limit', 'elapsed_runtime_total', 'elapsed_runtime_mean', 'elapsed_waiting_time_total', 'elapsed_waiting_time_mean']]\n",
    "\n",
    "y = df[['wait_time']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, shuffle = False)\n",
    "\n",
    "waiting_time_threshold = 6 * 60 * 60\n",
    "beta = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6b9a16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor(max_depth=5, max_features='sqrt', min_samples_leaf=250,\n",
      "                      min_samples_split=100, random_state=0)\n",
      "MCC: 0.0\n",
      "F1 score: 0.48516809492419255\n",
      "Precision score: 0.3202785030461271\n",
      "Recall score: 1.0\n",
      "FBeta score: 0.3336177474402731\n"
     ]
    }
   ],
   "source": [
    "# Waiting Times - Regression - Random Forest\n",
    "\n",
    "## Hyperparameter tuning with cross-validation\n",
    "\n",
    "cv_clf = RandomForestRegressor()\n",
    "param_search = {'n_estimators': [100], 'max_features': ['sqrt'], 'max_depth': [5, 10, 15], 'min_samples_leaf': [50, 100, 250], 'min_samples_split': [100, 200, 500], 'random_state': [0]}\n",
    "gsearch = GridSearchCV(estimator=cv_clf, cv=tscv, param_grid=param_search, scoring='neg_mean_absolute_error')\n",
    "gsearch.fit(X_train, y_train.values.ravel())\n",
    "print(gsearch.best_estimator_)\n",
    "\n",
    "## Test set evaluation\n",
    "\n",
    "y_pred = gsearch.predict(X_test)\n",
    "\n",
    "y_test_binary = y_test['wait_time'] > threshold_time\n",
    "y_pred_binary = y_pred > threshold_time\n",
    "\n",
    "print(\"MCC: \" + str(matthews_corrcoef(y_test_binary, y_pred_binary)))\n",
    "print(\"F1 score: \" + str(f1_score(y_test_binary, y_pred_binary)))\n",
    "print(\"Precision score: \" + str(precision_score(y_test_binary, y_pred_binary)))\n",
    "print(\"Recall score: \" + str(recall_score(y_test_binary, y_pred_binary)))\n",
    "print(\"FBeta score: \" + str(fbeta_score(y_test_binary, y_pred_binary, beta=beta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "560d5bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression()\n",
      "MCC: 0.2766183943534182\n",
      "F1 score: 0.5496711664935964\n",
      "Precision score: 0.4448179271708683\n",
      "Recall score: 0.7192028985507246\n",
      "FBeta score: 0.455029665587918\n"
     ]
    }
   ],
   "source": [
    "# Waiting Times - Regression - Linear Regression\n",
    "\n",
    "## Hyperparameter tuning with cross-validation\n",
    "\n",
    "cv_clf = LinearRegression()\n",
    "param_search = {}\n",
    "gsearch = GridSearchCV(estimator=cv_clf, cv=tscv, param_grid=param_search, scoring='neg_mean_absolute_error')\n",
    "gsearch.fit(X_train, y_train.values.ravel())\n",
    "print(gsearch.best_estimator_)\n",
    "\n",
    "## Test set evaluation\n",
    "\n",
    "y_pred = gsearch.predict(X_test)\n",
    "\n",
    "y_test_binary = y_test['wait_time'] > threshold_time\n",
    "y_pred_binary = y_pred > threshold_time\n",
    "\n",
    "print(\"MCC: \" + str(matthews_corrcoef(y_test_binary, y_pred_binary)))\n",
    "print(\"F1 score: \" + str(f1_score(y_test_binary, y_pred_binary)))\n",
    "print(\"Precision score: \" + str(precision_score(y_test_binary, y_pred_binary)))\n",
    "print(\"Recall score: \" + str(recall_score(y_test_binary, y_pred_binary)))\n",
    "print(\"FBeta score: \" + str(fbeta_score(y_test_binary, y_pred_binary, beta=beta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4cd6212c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingRegressor(max_features='sqrt', min_samples_leaf=250,\n",
      "                          min_samples_split=100, random_state=0)\n",
      "MCC: 0.3587105189308657\n",
      "F1 score: 0.5882352941176471\n",
      "Precision score: 0.5122606650990931\n",
      "Recall score: 0.6906702898550725\n",
      "FBeta score: 0.5201645264847512\n"
     ]
    }
   ],
   "source": [
    "# Waiting Times - Regression - Gradient Boosting\n",
    "\n",
    "## Hyperparameter tuning with cross-validation\n",
    "\n",
    "cv_clf = GradientBoostingRegressor()\n",
    "param_search = {'n_estimators': [100], 'max_features': ['sqrt'], 'min_samples_leaf': [50, 100, 250], 'min_samples_split': [100, 200, 500], 'random_state': [0]}\n",
    "gsearch = GridSearchCV(estimator=cv_clf, cv=tscv, param_grid=param_search, scoring='neg_mean_absolute_error')\n",
    "gsearch.fit(X_train, y_train.values.ravel())\n",
    "print(gsearch.best_estimator_)\n",
    "\n",
    "## Test set evaluation\n",
    "\n",
    "y_pred = gsearch.predict(X_test)\n",
    "\n",
    "y_test_binary = y_test['wait_time'] > threshold_time\n",
    "y_pred_binary = y_pred > threshold_time\n",
    "\n",
    "print(\"MCC: \" + str(matthews_corrcoef(y_test_binary, y_pred_binary)))\n",
    "print(\"F1 score: \" + str(f1_score(y_test_binary, y_pred_binary)))\n",
    "print(\"Precision score: \" + str(precision_score(y_test_binary, y_pred_binary)))\n",
    "print(\"Recall score: \" + str(recall_score(y_test_binary, y_pred_binary)))\n",
    "print(\"FBeta score: \" + str(fbeta_score(y_test_binary, y_pred_binary, beta=beta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab8bcf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0173acab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waiting time classification\n",
    "df['should_wait_waiting_time_actual'] = df['wait_time'] < waiting_time_threshold\n",
    "y = df[['should_wait_waiting_time_actual']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bce35d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(max_depth=15, min_samples_leaf=50, min_samples_split=100,\n",
      "                       n_estimators=250, random_state=0)\n",
      "MCC: 0.40038652204653113\n",
      "F1 score: 0.9634071340713407\n",
      "Precision score: 0.9398530073496325\n",
      "Recall score: 0.9881722125847658\n",
      "FBeta score: 0.9425641297903781\n"
     ]
    }
   ],
   "source": [
    "# Waiting Times - Classification - Random Forest\n",
    "\n",
    "## Hyperparameter tuning with cross-validation\n",
    "\n",
    "cv_clf = RandomForestClassifier()\n",
    "param_search = {'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2'], 'max_depth': [5, 10, 15], 'min_samples_leaf': [50, 100, 250], 'min_samples_split': [100, 200, 500], 'random_state': [0]}\n",
    "gsearch = GridSearchCV(estimator=cv_clf, cv=tscv, param_grid=param_search, scoring=fbeta_scorer)\n",
    "gsearch.fit(X_train, y_train.values.ravel())\n",
    "print(gsearch.best_estimator_)\n",
    "\n",
    "## Test set evaluation\n",
    "\n",
    "y_pred = gsearch.predict(X_test)\n",
    "\n",
    "print(\"MCC: \" + str(matthews_corrcoef(y_test, y_pred)))\n",
    "print(\"F1 score: \" + str(f1_score(y_test, y_pred)))\n",
    "print(\"Precision score: \" + str(precision_score(y_test, y_pred)))\n",
    "print(\"Recall score: \" + str(recall_score(y_test, y_pred)))\n",
    "print(\"FBeta score: \" + str(fbeta_score(y_test, y_pred, beta=beta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69a65f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(max_features='sqrt', min_samples_leaf=250,\n",
      "                           min_samples_split=100, n_estimators=250,\n",
      "                           random_state=0)\n",
      "MCC: 0.582607966651537\n",
      "F1 score: 0.970556161395856\n",
      "Precision score: 0.958904109589041\n",
      "Recall score: 0.9824948746254534\n",
      "FBeta score: 0.9602603973053593\n"
     ]
    }
   ],
   "source": [
    "# Waiting Times - Classification - Gradient Boosting\n",
    "\n",
    "## Hyperparameter tuning with cross-validation\n",
    "\n",
    "cv_clf = GradientBoostingClassifier()\n",
    "param_search = {'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2'], 'min_samples_leaf': [50, 100, 250], 'min_samples_split': [100, 200, 500], 'random_state': [0]}\n",
    "gsearch = GridSearchCV(estimator=cv_clf, cv=tscv, param_grid=param_search, scoring=fbeta_scorer)\n",
    "gsearch.fit(X_train, y_train.values.ravel())\n",
    "print(gsearch.best_estimator_)\n",
    "\n",
    "## Test set evaluation\n",
    "\n",
    "y_pred = gsearch.predict(X_test)\n",
    "\n",
    "print(\"MCC: \" + str(matthews_corrcoef(y_test, y_pred)))\n",
    "print(\"F1 score: \" + str(f1_score(y_test, y_pred)))\n",
    "print(\"Precision score: \" + str(precision_score(y_test, y_pred)))\n",
    "print(\"Recall score: \" + str(recall_score(y_test, y_pred)))\n",
    "print(\"FBeta score: \" + str(fbeta_score(y_test, y_pred, beta=beta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6509979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.1, max_iter=1000, random_state=0)\n",
      "MCC: 0.0\n",
      "F1 score: 0.958216849263317\n",
      "Precision score: 0.9197853205686104\n",
      "Recall score: 1.0\n",
      "FBeta score: 0.9241459128123795\n"
     ]
    }
   ],
   "source": [
    "# Waiting Times - Classification - Logistic Regression\n",
    "\n",
    "## Hyperparameter tuning with cross-validation\n",
    "\n",
    "cv_clf = LogisticRegression()\n",
    "param_search = {'max_iter': [1000, 10000], 'C': [0.1, 0.01, 0.001, 0.0001], 'random_state': [0]}\n",
    "gsearch = GridSearchCV(estimator=cv_clf, cv=tscv, param_grid=param_search, scoring=fbeta_scorer)\n",
    "gsearch.fit(X_train, y_train.values.ravel())\n",
    "print(gsearch.best_estimator_)\n",
    "\n",
    "## Test set evaluation\n",
    "\n",
    "y_pred = gsearch.predict(X_test)\n",
    "\n",
    "print(\"MCC: \" + str(matthews_corrcoef(y_test, y_pred)))\n",
    "print(\"F1 score: \" + str(f1_score(y_test, y_pred)))\n",
    "print(\"Precision score: \" + str(precision_score(y_test, y_pred)))\n",
    "print(\"Recall score: \" + str(recall_score(y_test, y_pred)))\n",
    "print(\"FBeta score: \" + str(fbeta_score(y_test, y_pred, beta=beta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c17ec8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(n_neighbors=50)\n",
      "MCC: 0.0027762111699319997\n",
      "F1 score: 0.9575803402646502\n",
      "Precision score: 0.9198140615920977\n",
      "Recall score: 0.9985806655101719\n",
      "FBeta score: 0.9241018156844228\n"
     ]
    }
   ],
   "source": [
    "# Waiting Times - Classification - Nearest Neighbors\n",
    "\n",
    "## Hyperparameter tuning with cross-validation\n",
    "\n",
    "cv_clf = KNeighborsClassifier()\n",
    "param_search = {'n_neighbors': [5, 10, 15, 25, 50]}\n",
    "gsearch = GridSearchCV(estimator=cv_clf, cv=tscv, param_grid=param_search, scoring=fbeta_scorer)\n",
    "gsearch.fit(X_train, y_train.values.ravel())\n",
    "print(gsearch.best_estimator_)\n",
    "\n",
    "## Test set evaluation\n",
    "\n",
    "y_pred = gsearch.predict(X_test)\n",
    "\n",
    "print(\"MCC: \" + str(matthews_corrcoef(y_test, y_pred)))\n",
    "print(\"F1 score: \" + str(f1_score(y_test, y_pred)))\n",
    "print(\"Precision score: \" + str(precision_score(y_test, y_pred)))\n",
    "print(\"Recall score: \" + str(recall_score(y_test, y_pred)))\n",
    "print(\"FBeta score: \" + str(fbeta_score(y_test, y_pred, beta=beta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc5f43e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(var_smoothing=1e-11)\n",
      "MCC: 0.04268542482587055\n",
      "F1 score: 0.9569037339998485\n",
      "Precision score: 0.9205770912270476\n",
      "Recall score: 0.9962151080271251\n",
      "FBeta score: 0.9247070169546985\n"
     ]
    }
   ],
   "source": [
    "# Waiting Times - Classification - Naive Bayes\n",
    "\n",
    "## Hyperparameter tuning with cross-validation\n",
    "\n",
    "cv_clf = GaussianNB()\n",
    "param_search = {'var_smoothing': [1e-11, 1e-10, 1e-9]}\n",
    "gsearch = GridSearchCV(estimator=cv_clf, cv=tscv, param_grid=param_search, scoring=fbeta_scorer)\n",
    "gsearch.fit(X_train, y_train.values.ravel())\n",
    "print(gsearch.best_estimator_)\n",
    "\n",
    "## Test set evaluation\n",
    "\n",
    "y_pred = gsearch.predict(X_test)\n",
    "\n",
    "print(\"MCC: \" + str(matthews_corrcoef(y_test, y_pred)))\n",
    "print(\"F1 score: \" + str(f1_score(y_test, y_pred)))\n",
    "print(\"Precision score: \" + str(precision_score(y_test, y_pred)))\n",
    "print(\"Recall score: \" + str(recall_score(y_test, y_pred)))\n",
    "print(\"FBeta score: \" + str(fbeta_score(y_test, y_pred, beta=beta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c73e591",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
