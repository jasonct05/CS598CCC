{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64bf1ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries required to do Neural Nets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51666851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mustang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e35bf221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows prior to cleaning: 19018575\n",
      "rows after to cleaning: 18147350\n",
      "      user_ID  group_ID                submit_time                 start_time  \\\n",
      "1000      354       357  2011-10-28 13:13:27-06:00  2011-10-28 13:13:45-06:00   \n",
      "1001      354       357  2011-10-28 13:13:34-06:00  2011-10-28 13:13:45-06:00   \n",
      "1002      354       357  2011-10-28 13:13:40-06:00  2011-10-28 13:13:45-06:00   \n",
      "1003      354       357  2011-10-28 13:13:47-06:00  2011-10-28 13:14:16-06:00   \n",
      "1004      427       435  2011-10-28 13:20:06-06:00  2011-10-28 13:20:28-06:00   \n",
      "\n",
      "                       end_time  wallclock_limit job_status  node_count  \\\n",
      "1000  2011-10-28 13:19:36-06:00           7200.0  COMPLETED           8   \n",
      "1001  2011-10-28 13:19:43-06:00           7200.0  COMPLETED           8   \n",
      "1002  2011-10-28 13:19:45-06:00           7200.0  COMPLETED           8   \n",
      "1003  2011-10-28 13:20:31-06:00           7200.0  COMPLETED           8   \n",
      "1004  2011-10-28 13:20:31-06:00           7200.0  COMPLETED           1   \n",
      "\n",
      "      tasks_requested  runtime  \n",
      "1000              192    351.0  \n",
      "1001              192    358.0  \n",
      "1002              192    360.0  \n",
      "1003              192    375.0  \n",
      "1004               24      3.0  \n"
     ]
    }
   ],
   "source": [
    "# Import Training CSV\n",
    "df = pd.read_csv(r\"../Dataset/mustang_release_v1.0beta.csv\")\n",
    "\n",
    "# Data Cleaning Step\n",
    "print(\"rows prior to cleaning: \" + str(df.size))\n",
    "\n",
    "# Drop first 1000 rows\n",
    "df = df.iloc[1000: , :]\n",
    "\n",
    "# Convert wallclock_limit to be in seconds\n",
    "df['wallclock_limit'] = pd.to_timedelta(df['wallclock_limit']).dt.total_seconds()\n",
    "\n",
    "# Add Runtime Attribute\n",
    "df['start_time'] = pd.to_datetime(df['start_time'])\n",
    "df['end_time'] = pd.to_datetime(df['end_time'])\n",
    "df['runtime'] = (df['end_time'] - df['start_time']).dt.total_seconds()\n",
    "\n",
    "# We only care about jobs that are completed\n",
    "df = df[df.job_status == 'COMPLETED']\n",
    "\n",
    "# Filter to only contain rows that have non-zero runtime\n",
    "df = df.dropna(subset=['start_time'])\n",
    "df = df.dropna(subset=['end_time'])\n",
    "df = df[df.runtime != 0]\n",
    "\n",
    "print('rows after to cleaning: ' + str(df.size))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2409ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data to Attributes(X) and Labels(y).\n",
    "X = df[['user_ID', 'group_ID', 'wallclock_limit', 'node_count', 'tasks_requested']]\n",
    "y = df[['runtime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "645b0f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# define the keras model\n",
    "# Tune hidden_layer_size + max_iter. Monitor training progress.\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(128,64,32), activation='relu', solver='adam', max_iter=500)\n",
    "mlp.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "y_pred = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2586dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 1633.7130188481042\n",
      "Mean Squared Error: 31906266.85329935\n",
      "Root Mean Squared Error: 5648.563255669476\n"
     ]
    }
   ],
   "source": [
    "# metrics\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aeaca381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         actual   Predicted\n",
      "2011656     6.0  928.871723\n",
      "682778     85.0  282.542249\n",
      "937396    195.0  297.935724\n",
      "2057438    45.0  203.295402\n",
      "664626     96.0  235.517403\n"
     ]
    }
   ],
   "source": [
    "df_result = pd.DataFrame({'actual': y_test[\"runtime\"], 'Predicted': y_pred})\n",
    "print(df_result.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fc83256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7222a389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows prior to cleaning: 277607\n",
      "rows after to cleaning: 192408\n",
      "      user_ID  group_ID                submit_time                 start_time  \\\n",
      "1000        1         1  2016-02-05 10:15:47-07:00  2016-02-05 10:15:48-07:00   \n",
      "1001        1         1  2016-02-05 10:15:48-07:00  2016-02-05 10:15:49-07:00   \n",
      "1002        1         1  2016-02-05 10:15:49-07:00  2016-02-05 10:15:50-07:00   \n",
      "1003        1         1  2016-02-05 10:15:50-07:00  2016-02-05 10:15:51-07:00   \n",
      "1004        1         1  2016-02-05 10:15:51-07:00  2016-02-05 10:15:52-07:00   \n",
      "\n",
      "                  dispatch_time                 queue_time  \\\n",
      "1000  2016-02-05 10:15:48-07:00  2016-02-05 10:15:47-07:00   \n",
      "1001  2016-02-05 10:15:49-07:00  2016-02-05 10:15:48-07:00   \n",
      "1002  2016-02-05 10:15:50-07:00  2016-02-05 10:15:49-07:00   \n",
      "1003  2016-02-05 10:15:51-07:00  2016-02-05 10:15:50-07:00   \n",
      "1004  2016-02-05 10:15:52-07:00  2016-02-05 10:15:51-07:00   \n",
      "\n",
      "                       end_time  wallclock_limit job_status  node_count  \\\n",
      "1000  2016-02-05 14:12:32-07:00          20700.0     JOBEND           1   \n",
      "1001  2016-02-05 14:14:23-07:00          20700.0     JOBEND           1   \n",
      "1002  2016-02-05 14:13:48-07:00          20700.0     JOBEND           1   \n",
      "1003  2016-02-05 14:16:17-07:00          20700.0     JOBEND           1   \n",
      "1004  2016-02-05 14:12:32-07:00          20700.0     JOBEND           1   \n",
      "\n",
      "      tasks_requested  runtime  \n",
      "1000                1  14204.0  \n",
      "1001                1  14314.0  \n",
      "1002                1  14278.0  \n",
      "1003                1  14426.0  \n",
      "1004                1  14200.0  \n"
     ]
    }
   ],
   "source": [
    "# Import Training CSV\n",
    "df = pd.read_csv(\"../Dataset/trinity_formatted_release_v1.0beta.csv\")\n",
    "\n",
    "# Data Cleaning Step\n",
    "print(\"rows prior to cleaning: \" + str(df.size))\n",
    "\n",
    "# Drop first 1000 rows\n",
    "df = df.iloc[1000: , :]\n",
    "\n",
    "# Convert wallclock_limit to be in seconds\n",
    "df['wallclock_limit'] = pd.to_timedelta(df['wallclock_limit']).dt.total_seconds()\n",
    "\n",
    "# Add Runtime Attribute\n",
    "df['start_time'] = pd.to_datetime(df['start_time'])\n",
    "df['end_time'] = pd.to_datetime(df['end_time'])\n",
    "df['runtime'] = (df['end_time'] - df['start_time']).dt.total_seconds()\n",
    "\n",
    "# We only care about jobs that are completed\n",
    "df = df[df.job_status == 'JOBEND']\n",
    "\n",
    "# Filter to only contain rows that have non-zero runtime\n",
    "df = df.dropna(subset=['start_time'])\n",
    "df = df.dropna(subset=['end_time'])\n",
    "df = df[df.runtime != 0]\n",
    "\n",
    "print('rows after to cleaning: ' + str(df.size))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8af7f526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data to Attributes(X) and Labels(y).\n",
    "X = df[['user_ID', 'group_ID', 'wallclock_limit', 'node_count', 'tasks_requested']]\n",
    "y = df[['runtime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1d3cf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# define the keras model\n",
    "# Tune hidden_layer_size + max_iter. Monitor training progress.\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(128,64,32), activation='relu', solver='adam', max_iter=500)\n",
    "mlp.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "y_pred = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4a9e253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 2396.4955317558547\n",
      "Mean Squared Error: 25783779.974758424\n",
      "Root Mean Squared Error: 5077.773131477855\n"
     ]
    }
   ],
   "source": [
    "# metrics\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "228fa9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        actual     Predicted\n",
      "3961   13879.0  11556.503733\n",
      "6077     104.0    253.168816\n",
      "9830    8137.0   2182.878488\n",
      "15981    427.0    637.694131\n",
      "5494      25.0   1367.620697\n"
     ]
    }
   ],
   "source": [
    "df_result = pd.DataFrame({'actual': y_test[\"runtime\"], 'Predicted': y_pred})\n",
    "print(df_result.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb6d323",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
